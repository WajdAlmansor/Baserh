import SwiftUI
import AVFoundation
import Firebase
import FirebaseAuth

struct CameraPreview: UIViewRepresentable {
    class CameraView: UIView {
        
        override class var layerClass: AnyClass {
            return AVCaptureVideoPreviewLayer.self
        }

        var session: AVCaptureSession? {
            get {
                (layer as! AVCaptureVideoPreviewLayer).session
            }
            set {
                let previewLayer = layer as! AVCaptureVideoPreviewLayer
                previewLayer.session = newValue
                previewLayer.videoGravity = .resizeAspectFill
            }
        }
    }

    var session: AVCaptureSession

    func makeUIView(context: Context) -> CameraView {
        let cameraView = CameraView()
        cameraView.session = session
        return cameraView
    }

    func updateUIView(_ uiView: CameraView, context: Context) {
        uiView.session = session
    }
}

import AVFoundation
import Vision
import Combine
import FirebaseDatabase

class CameraManager: NSObject, ObservableObject, AVCaptureVideoDataOutputSampleBufferDelegate {
    var session: AVCaptureSession = AVCaptureSession()
    private var videoOutput = AVCaptureVideoDataOutput()
    private var detectionRequest: VNCoreMLRequest?
    private let speechSynthesizer = AVSpeechSynthesizer()
    private var realtimeDatabaseRef = Database.database().reference().child("defaultVoice") // ğŸ”¥ Ø±Ø¨Ø· Ø§Ù„Ø±ÙŠÙ„ ØªØ§ÙŠÙ…
    private var lastDetectionTime: Date = Date(timeIntervalSince1970: 0)

    @Published var detectedObject: String = "Detecting..." {
        didSet {
            speak(detectedObject)
        }
    }
    
    private let arabicTranslations: [String: String] = [
        "wall clock" : "Ø³Ø§Ø¹Ø© Ø­Ø§Ø¦Ø·",
        "traffic light, traffic signal, stoplight" : "Ø¥Ø´Ø§Ø±Ø© Ù…Ø±ÙˆØ±",
        "sandal" : "Ø¬Ø²Ù…Ù‡",
        "abaya" : "Ø¹Ø¨Ø§ÙŠÙ‡",
        "backpack, back pack, knapsack, packsack, rucksack, haversack" : "Ø­Ù‚ÙŠØ¨Ø©",
        "mailbag, postbag" : "Ø­Ù‚ÙŠØ¨Ø©",
        "carton" : "ÙƒØ±ØªÙˆÙ†",
        "analog clock" : "Ø³Ø§Ø¹Ø© Ø­Ø§Ø¦Ø·",
        "straweberry" : "ÙØ±Ø§ÙˆÙ„Ù‡",
        "car" : "Ø³ÙŠØ§Ø±Ù‡",
        "orange" : "Ø¨Ø±ØªÙ‚Ø§Ù„",
        "remote control, remote" : "Ø¬Ù‡Ø§Ø² ØªØ­ÙƒÙ…",
        "pineapple" : "Ø§Ù†Ø§Ù†Ø§Ø³",
        "running shoe" : "Ø¬Ø²Ù…Ø© Ø±ÙŠØ§Ø¶ÙŠØ©",
        "plastic bag" : "ÙƒÙŠØ³ Ø¨Ù„Ø§Ø³ØªÙŠÙƒ",
        "toilet tissue, toilet paper, bathroom tissue" : "Ù…Ù†Ø§Ø¯ÙŠÙ„",
        "computer keyboard, keypad" : "Ù„ÙˆØ­Ø© Ù…ÙØ§ØªÙŠØ­",
        "mouse, computer mouse" : "ÙØ£Ø± Ø­Ø§Ø³ÙˆØ¨",
        "desktop computer" : "ÙƒÙ…Ø¨ÙŠÙˆØªØ±",
        "spotlight, spot" : "Ø¶ÙˆØ¡",
        "Abaya" : "Ø¹Ø¨Ø§ÙŠÙ‡",
        "cellphone" : "Ø¬ÙˆØ§Ù„",
        "banana" : "Ù…ÙˆØ²",
        "apple" : "ØªÙØ§Ø­",
        "desktop" : "Ù…ÙƒØªØ¨",
        "website" : "Ù…ÙˆÙ‚Ø¹",
        "monitor" : "Ø´Ø§Ø´Ø©",
        "notebook, notebook computer" : "Ø¬Ù‡Ø§Ø² Ù„ÙˆØ­ÙŠ",
        "keyboard": "Ù„ÙˆØ­Ø© Ù…ÙØ§ØªÙŠØ­",
        "mouse": "ÙØ£Ø±Ø©",
        "bottle": "Ø²Ø¬Ø§Ø¬Ø©",
        "chair": "ÙƒØ±Ø³ÙŠ",
        "table": "Ø·Ø§ÙˆÙ„Ø©",
        "phone": "Ù‡Ø§ØªÙ",
        "pen": "Ù‚Ù„Ù…",
        "book": "ÙƒØªØ§Ø¨",
        "laptop, laptop computer": "Ø­Ø§Ø³ÙˆØ¨ Ù…Ø­Ù…ÙˆÙ„",
        "tv": "ØªÙ„ÙØ§Ø²",
        "cup": "ÙƒÙˆØ¨",
        "lamp": "Ù…ØµØ¨Ø§Ø­",
        "shoe": "Ø­Ø°Ø§Ø¡",
        "bag": "Ø­Ù‚ÙŠØ¨Ø©",
        "glasses": "Ù†Ø¸Ø§Ø±Ø§Øª",
        "watch": "Ø³Ø§Ø¹Ø©",
        "fan": "Ù…Ø±ÙˆØ­Ø©",
        "door": "Ø¨Ø§Ø¨",
        "window": "Ù†Ø§ÙØ°Ø©",
        "plant": "Ù†Ø¨ØªØ©",
        "remote": "Ø¬Ù‡Ø§Ø² ØªØ­ÙƒÙ…",
        "pillow": "ÙˆØ³Ø§Ø¯Ø©",
        "bed": "Ø³Ø±ÙŠØ±",
        "mirror": "Ù…Ø±Ø¢Ø©",
        "clock": "Ø³Ø§Ø¹Ø© Ø­Ø§Ø¦Ø·",
        "towel": "Ù…Ù†Ø´ÙØ©",
        "toothbrush": "ÙØ±Ø´Ø§Ø© Ø£Ø³Ù†Ø§Ù†",
        "soap": "ØµØ§Ø¨ÙˆÙ†",
        "sink": "Ù…ØºØ³Ù„Ø©",
        "barber chair" : "ÙƒØ±Ø³ÙŠ",
        "water bottle" : "Ù‚Ø§Ø±ÙˆØ±Ø© Ù…ÙŠØ§Ù‡",
        "paper towel" : "Ù…Ù†Ø§Ø¯ÙŠÙ„",
        "rubber eraser, rubber, pencil eraser" : "Ù‚Ù„Ù… Ø­Ø¨Ø±"
    ]

    @Published var defaultVoice: (name: String, volume: Float, rate: Float, pitch: Float) = ("Default", 0.5, 0.5, 1.0)
    private var cancellable: AnyCancellable?
    private var isLocked: Bool = false

    override init() {
        super.init()
        setupSession()
        setupObjectDetection()
        loadUserDefaultVoice()
        observeDefaultVoice()
        NotificationCenter.default.addObserver(self, selector: #selector(loadUserDefaultVoice), name: .defaultVoiceChanged, object: nil)
        print("ğŸ“‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙ… ÙÙŠ UserDefaults: \(UserDefaults.standard.dictionaryRepresentation())")
    }

    private func setupSession() {
        session.beginConfiguration()
        session.sessionPreset = .photo

        guard let videoDevice = AVCaptureDevice.default(for: .video),
              let videoInput = try? AVCaptureDeviceInput(device: videoDevice) else {
            print("Error: Unable to access the camera.")
            return
        }

        guard session.canAddInput(videoInput) else { return }
        session.addInput(videoInput)

        videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "cameraQueue"))
        guard session.canAddOutput(videoOutput) else { return }
        session.addOutput(videoOutput)

        session.commitConfiguration()
    }
    
//Ai
    private func setupObjectDetection() {
        do {
            let coreMLModel = try MobileNetV2(configuration: MLModelConfiguration()).model
            let visionModel = try VNCoreMLModel(for: coreMLModel)

            detectionRequest = VNCoreMLRequest(model: visionModel, completionHandler: handleDetection)
            print("MobileNetV2 object detection setup complete.")
        } catch {
            print("Error loading MobileNetV2 Core ML model: \(error.localizedDescription)")
        }
    }

    private func handleDetection(request: VNRequest, error: Error?) {
        guard !isLocked,
              let results = request.results as? [VNClassificationObservation],
              let topResult = results.first else {
            return
        }

        DispatchQueue.main.async {
            let identifier = topResult.identifier
            
            let arabicWord = self.arabicTranslations[identifier] ?? identifier
        
            self.detectedObject = arabicWord
            self.isLocked = true
        }
    }

    func startSession() {
        DispatchQueue.global(qos: .userInitiated).async {
            if !self.session.isRunning {
                print("ğŸ“· ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")
                self.session.startRunning()
            }
        }
    }

    func stopSession() {
        DispatchQueue.global(qos: .userInitiated).async {
            if self.session.isRunning {
                print("ğŸ›‘ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")
                self.session.stopRunning()
            }
        }
    }

    func resetDetection() {
        DispatchQueue.main.async {
            self.isLocked = false
            self.detectedObject = "Detecting..."
        }
    }

    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        // âœ… Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙ‚Ø· ÙƒÙ„ 1 Ø«Ø§Ù†ÙŠØ©
        let now = Date()
        guard now.timeIntervalSince(self.lastDetectionTime) > 1.0 else { return }
        self.lastDetectionTime = now

        guard !isLocked else { return }

        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer),
              let detectionRequest = detectionRequest else {
            return
        }

        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
        do {
            try handler.perform([detectionRequest])
        } catch {
            print("Error performing Vision request: \(error.localizedDescription)")
        }
    }



    /// âœ… **Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØµÙˆØª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø³ÙˆØ§Ø¡ Ù…Ù† Realtime Database Ø£Ùˆ Ù…Ù† UserDefaults**
    func observeDefaultVoice() {
        // ğŸ”¥ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…Ù† Firebase Realtime Database
        realtimeDatabaseRef.observe(.value) { snapshot in
            if let value = snapshot.value as? [String: Any] {
                DispatchQueue.main.async {
                    let globalVoice = (
                        name: value["name"] as? String ?? "Default",
                        volume: value["volume"] as? Float ?? 0.5,
                        rate: value["rate"] as? Float ?? 0.5,
                        pitch: value["pitch"] as? Float ?? 1.0
                    )
                    
                    // âœ… Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù„Ø¯Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØµÙˆØª Ù…Ø­Ù„ÙŠØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØµÙˆØª Ø§Ù„Ø¹Ø§Ù…
                    if UserDefaults.standard.dictionary(forKey: "userDefaultVoice") == nil {
                        self.defaultVoice = globalVoice
                        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…Ù† Realtime Database: \(self.defaultVoice.name)")
                    }
                }
            }
        }
        
        // âœ… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…Ù† `UserDefaults` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `NotificationCenter`
        NotificationCenter.default.addObserver(self, selector: #selector(loadUserDefaultVoice), name: .defaultVoiceChanged, object: nil)

        // ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØª Ù…Ù† UserDefaults Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
        loadUserDefaultVoice()
    }

    /// âœ… **ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§**
    @objc private func loadUserDefaultVoice() {
        DispatchQueue.main.async {
            let name = UserDefaults.standard.string(forKey: "defaultVoiceName") ?? "Default"
            let volume = UserDefaults.standard.float(forKey: "defaultVoiceVolume")
            let rate = UserDefaults.standard.float(forKey: "defaultVoiceRate")
            let pitch = UserDefaults.standard.float(forKey: "defaultVoicePitch")
            let language = UserDefaults.standard.string(forKey: "defaultVoiceLanguage") ?? "ar-SA"

            self.defaultVoice = (name: name, volume: volume, rate: rate, pitch: pitch)

            print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…Ù† UserDefaults: \(self.defaultVoice)")
        }
    }


    /// âœ… **ØªØ­Ø¯ÙŠØ« Ø§Ù„ØµÙˆØª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…Ø­Ù„ÙŠÙ‹Ø§ ÙÙ‚Ø·**
    func updateDefaultVoice(name: String, volume: Float, rate: Float, pitch: Float, language: String) {
        UserDefaults.standard.set(name, forKey: "defaultVoiceName")
        UserDefaults.standard.set(volume, forKey: "defaultVoiceVolume")
        UserDefaults.standard.set(rate, forKey: "defaultVoiceRate")
        UserDefaults.standard.set(pitch, forKey: "defaultVoicePitch")
        UserDefaults.standard.set(language, forKey: "defaultVoiceLanguage")

        NotificationCenter.default.post(name: .defaultVoiceChanged, object: nil)

        DispatchQueue.main.async {
            self.defaultVoice = (name: name, volume: volume, rate: rate, pitch: pitch)
            print("âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„ØµÙˆØª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…Ø­Ù„ÙŠÙ‹Ø§ Ø¥Ù„Ù‰: \(self.defaultVoice)")
        }
    }


    /// âœ… **Ø§Ù„ØªØ­Ø¯Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØµÙˆØª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ**
//    private func speak(_ text: String) {
//        let utterance = AVSpeechUtterance(string: text)
//        utterance.voice = AVSpeechSynthesisVoice(language: "ar-SA")
//        utterance.volume = defaultVoice.volume
//        utterance.rate = defaultVoice.rate
//        utterance.pitchMultiplier = defaultVoice.pitch
//
//        speechSynthesizer.speak(utterance)
//    }
    
    private func speak(_ text: String) {
        let language = arabicTranslations.values.contains(text) ? "ar-SA" : "en-US"

        let utterance = AVSpeechUtterance(string: text)
        utterance.voice = AVSpeechSynthesisVoice(language: language)
        utterance.volume = defaultVoice.volume
        utterance.rate = defaultVoice.rate
        utterance.pitchMultiplier = defaultVoice.pitch

        speechSynthesizer.speak(utterance)
    }

}

/// âœ… **ØªØ¹Ø±ÙŠÙ NotificationCenter Key**
extension Notification.Name {
    static let defaultVoiceChanged = Notification.Name("defaultVoiceChanged")
}

